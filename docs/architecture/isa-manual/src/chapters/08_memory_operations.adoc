[[memory]]
== Memory operations (loads, stores, atomics)

This chapter summarizes the addressing conventions used by the scalar memory instruction families. Refer to the
instruction reference for the full set of encodings and per-form notes.

[[memory-model]]
=== Memory model and ordering (strict v0.3 current)

Linx uses **TSO** as the architectural memory model. The ordering contract is expressed in Linx terms:

* A **memory operation** is a load, store, atomic read-modify-write, or fence.
* **Program order** is the architectural order of committed memory operations
  on a single LXCPU.
* A **device/MMIO** access is any access to regions defined by the platform
  as device memory (including UART-style MMIO used by bring-up).

Strict profile contract:

* Storeâ†’store order is preserved for all addresses (including `TSTORE` beats).
* Loads may observe store-buffer behavior consistent with TSO.
* Fences/atomics (`.aq/.rl/.aqrl`, `FENCE.D`, `FENCE.I`) provide additional ordering constraints.

NOTE: Implementations may be stronger than TSO, but software must not depend on stronger-than-TSO behavior unless a
platform profile states so.

[[memory-channels]]
=== BCC and MTC channels (strict v0.3)

Strict v0.3 explicitly models two architectural memory issue channels:

* **BCC channel**: scalar `load/store/atomic/fence` operations and ordering point for scalar blocks.
* **MTC channel**: tile-oriented memory blocks (`TLOAD`, `TSTORE`, `TPREFETCH`) and staged MCALL-like memory mode.

Legacy naming note: historical `BSTART.MCALL` text maps to strict-v0.3 `BSTART.MPAR`/`BSTART.MSEQ`.

`TMOV` is a tile-state move operation in typed `BSTART.TMA` blocks, but it is not a memory operation and is therefore
outside the MTC memory-ordering contract in this chapter.

Rules:

* In normal tile mode, BCC and MTC requests share one architectural TSO ordering domain.
* Entering MCALL-like mode requires an acquire-style boundary before MTC-only execution begins.
* Exiting MCALL-like mode requires a release-style boundary before subsequent scalar blocks execute.
* While MCALL-like mode is active, scalar memory issue is architecturally closed for correctness.

[[memory-tload-tstore]]
=== TLOAD/TSTORE semantics

`TLOAD` and `TSTORE` are template-style tile memory operations. The staged architectural model is:

* Address footprint is 2D and derived from descriptor state:
  - dimensions from `LB0/LB1/LB2` (`B.DIM` / `C.B.DIM*`)
  - layout-conversion selector and pad policy from `B.ARG`
  - GPR bindings from one-or-more `B.IOR` descriptors
  - tile IO bindings and transfer size from one-or-more `B.IOT/B.IOTI`
  - element data type from `BSTART.TMA`

Layout and dimension contract (strict v0.3 current profile for TMA):

* The four architectural tile linearization forms are:
  - `ND` (row-major),
  - `DN` (column-major),
  - `NZ` (inner-block row-major + inter-block column-major),
  - `ZN` (inner-block column-major + inter-block row-major).
* `B.ARG` encodes one unified format selector for the current TMA tile-memory operation.
* The only legal format selectors are:
  - `NORM.normal` (source and destination layout are the same),
  - `ND2NZ.normal`,
  - `ND2ZN.normal`,
  - `DN2NZ.normal`,
  - `DN2ZN.normal`.
* For both `TLOAD` and `TSTORE`, `NZ`/`ZN` always denotes the TR-side layout; GM-side layout is `ND` or `DN`.
* `LB0/LB1/LB2` are interpreted for TLOAD/TSTORE as:
  - `LB0`: GM-side inner element count
  - `LB1`: GM-side outer element count
  - `LB2`: TR-side inner element count
* TR-side outer element count is derived as:
  - `TR_outer = TileElements / LB2`
  - `TileElements = TileBytes / element_size`, where `TileBytes` comes from the active `B.IOTI`/size descriptor.
* When GM-side and TR-side logical matrix sizes differ, TR-side size MUST be greater than or equal to GM-side size.
* `NZ`/`ZN` are TR-side-only layouts in the strict profile:
  - GM-side source/destination layout in TLOAD/TSTORE MUST be `ND` or `DN`.
* For `NZ`/`ZN` TR-side tiles, the following constraints are required:
  - `TR_inner * element_size` MUST be a multiple of `32B`.
  - `TR_outer` MUST be a multiple of `16`.
* For `ND`/`DN`, implementations SHOULD optimize common power-of-two dimension values (for example: `64/128/256`);
  software tests SHOULD cover those values preferentially while retaining a small number of non-power-of-two cases.

Padding contract (TLOAD):

* If TR-side logical size is larger than GM-side logical size, elements outside the GM-side footprint are synthesized in
  TR according to the pad policy encoded in `B.ARG`.
* Required pad policies are:
  - `Null`: implementation-defined non-deterministic fill (bring-up model may use deterministic pseudo-random fill),
  - `Zero`: fill with numeric zero,
  - `Max`: fill with the maximum representable value of the selected element type,
  - `Min`: fill with the minimum representable value of the selected element type.
* TSTORE does not synthesize padding data; values are read from TR and written to GM according to the selected
  source/destination layouts and the effective GM footprint.

Bring-up (auto-mode) binding convention for `B.IOR` in `TLOAD/TSTORE`:

* The effective base pointer register is taken from the most recent `B.IOR` in the header:
  - prefer `RegSrc1` when it is non-zero,
  - otherwise fall back to `RegSrc0` (legacy/base-only streams).
* When both are present, `RegSrc0` is treated as a stride/leading-dimension register and `RegSrc1` as the base pointer.
  Canonical disassembly prints the raw register list in encoding order (`[RegSrc0,RegSrc1,...]`), so the base pointer
  commonly appears as the *second* entry (for example `B.IOR [s0,a6],[]`).
* A single `TLOAD`/`TSTORE` may decompose into multiple internal memory beats; completion is not atomic to observers.
* Internal beats of one operation are not guaranteed to issue in source-program order.
* Unaligned shared-memory access is allowed in staged profile unless tightened by platform policy.
* Group-overlap contracts that are unresolved in raw v0.3 material are deferred from canonical spec behavior.

Canonical strict-v0.3 profile constraints:

* `B.ARG + B.IOR` are mandatory in descriptor-rich `TLOAD/TSTORE` headers.
* TMA execution state is not sourced from `B.ATTR`; `TLOAD/TSTORE` consume only `BSTART/B.ARG/B.DIM/B.IOR/B.IOT`.
* Repeated `B.IOR` and repeated `B.IOT/B.IOTI` are legal and are consumed in header order.
* `B.IOT` dynamic-size descriptors are legal only when `RegSrc` resolves to strict `512B..4KB`; runtime rejects invalid
  dynamic sizes.
* For bring-up code size, emit `C.B.DIMI` for `LB0/LB1/LB2` when the value fits in `imm8` (and `RegSrc=zero`).
* Unresolved raw contracts (`group=2`, broad fixed-pipe/TCVT metadata flows) are explicitly deferred from canonical behavior.

Preferred strict-v0.3 textual form for tile memory blocks is:

* `BSTART.TLOAD <DataType>` for tile loads (encoding: `BSTART.TMA Function=0`)
* `BSTART.TSTORE <DataType>` for tile stores (encoding: `BSTART.TMA Function=1`)
* Compatibility PAR-style forms and mixed TMA forms are accepted for input only and normalized to typed forms.

For completeness: strict-v0.3 also permits non-memory typed TMA headers `BSTART.TMOV <DataType>`
(encoding: `BSTART.TMA Function=2`), but those are governed by <<blockisa-tile>> (not by memory-operation semantics).

`B.IOTI` descriptor rendering uses `last` for the terminal descriptor and prints transfer footprints in KiB (for example
`->t<1KB>`, `->acc<4KB>`). The strict architectural range is `512B..4KB`.

[[memory-mcall]]
=== MCALL staged semantics

MCALL-like behavior remains staged through `.MPAR/.MSEQ` execution modes (`MCALL` is a legacy alias name):

* Execution is non-speculative until declared input dependencies (`B.IOT`/`B.IOTI`) are resolved.
* Prior side-effecting tile stores must be globally ordered before MCALL-like execution starts (TSO-preserving barrier
  requirement).
* Detailed VREG-only local register semantics from raw notes are deferred until fully reconciled.

[[memory-types]]
==== Memory types (Normal vs Device)

Linx platforms classify memory addresses into types. This v0.3 staged manual uses two
high-level types:

* **Normal**: cacheable memory used for code and data (SRAM/DRAM).
* **Device/MMIO**: strongly ordered, side-effecting regions used to communicate
  with devices (UART-style MMIO, interrupt controllers, etc.).

If a platform exposes additional subtypes (e.g. non-cacheable Normal, write
combining, etc.), it MUST specify how they map onto the `FENCE.D` classes (`R`,
`W`, `O`) and what extra ordering they guarantee.

[[memory-observability]]
==== Observability and order (informal model)

For bring-up, it is useful to reason about *when* a memory operation can be
observed by other agents (other LXCPUs or devices):

* A memory operation is **committed** when it becomes architecturally visible
  on the issuing LXCPU (i.e. it is past the point where a precise exception
  could cancel it).
* A memory operation is **observed** by another agent when the effects become
  visible to that agent (e.g. a load reads a value written by another LXCPU, or
  a device sees an MMIO write).

The memory model constrains the relative order of **observations** implied by
program order and by fences/atomics. It does not require a specific
micro-architecture (OOO vs in-order).

[[memory-atomics-ordering]]
==== Atomic qualifiers (`.aq` / `.rl` / `.aqrl`)

Many atomic instruction families accept ordering qualifiers:

* `.aq` (**acquire**) orders the atomic operation before subsequent loads/stores
  (and subsequent atomics) in program order on the same LXCPU.
* `.rl` (**release**) orders prior loads/stores (and prior atomics) before the
  atomic operation in program order on the same LXCPU.
* `.aqrl` combines acquire and release ordering.

NOTE: These qualifiers define the **architectural** ordering contract. A micro-
architecture may implement stronger ordering, but software MUST NOT rely on
stronger behavior unless the platform profile states so.

[[memory-endianness]]
=== Endianness and alignment

The v0.3 staged Linx toolchain and emulator model are **little-endian**: the least-significant byte of a multi-byte value is
stored at the lowest address.

Alignment rules are profile-defined. Unless a profile explicitly enables split/misaligned accesses, software should
assume that misaligned loads/stores MAY raise an exception.

[[memory-addressing]]
=== Addressing conventions

Many load/store forms use one of two base addressing styles:

* *Register-offset:* `[Base, Index<type><<shamt>]`
* *Immediate-offset:* `[Base, imm]` (scaled or unscaled depending on the mnemonic family)

The bring-up profile describes common register-offset semantics as:

* `addr = Base + (Index << shamt)`

Some mnemonics use *fixed* scaling implied by the access width (byte/halfword/word/dword), while others expose explicit
`shamt` fields.

[[memory-srcRtype]]
=== `SrcRType` (`.sw` / `.uw` / `.neg`)

Some register-offset forms annotate the index operand with a suffix that affects how the index value is interpreted:

* `.sw`: treat as signed word
* `.uw`: treat as unsigned word
* `.neg`: treat as negated (used by some ALU/compound forms)

These suffixes are part of the assembly syntax and correspond to encoded fields in certain instruction formats.

[[memory-scaled-immediates]]
=== Scaled vs unscaled immediates

Several immediate-offset families encode an immediate that is implicitly scaled by the access size:

* byte: scale 1 (`<< 0`)
* halfword: scale 2 (`<< 1`)
* word: scale 4 (`<< 2`)
* doubleword: scale 8 (`<< 3`)

Unscaled variants use a `.U` suffix (e.g. `LWI.U`, `LDI.U`) and use the immediate value in
bytes without the implicit shift.

[[memory-pcrel]]
=== PC-relative loads/stores

The catalog includes `*.PCR` forms that compute an address relative to the current PC/TPC (as defined by the draft).
These forms are used by toolchains for efficient access to nearby constants and for position-independent code patterns.

[[memory-atomics]]
=== Atomics (overview)

The v0.3 staged catalog includes load-reserved/store-conditional (`LR.*`/`SC.*`) and read-modify-write style atomic operations
with qualifier suffixes (e.g. `.aq`, `.rl`). The precise memory ordering semantics are defined by the ISA draft
(`isa/v0.3/`) and the architectural model implemented by the toolchain/QEMU/RTL.

Bring-up model (ordering qualifiers):

* `.aq` (*acquire*): prevents later loads/stores from being observed before the atomic operation.
* `.rl` (*release*): prevents earlier loads/stores from being observed after the atomic operation.
* `.aqrl`: combines acquire and release.

In Arm-like terms, `.aq` is analogous to an acquire barrier on the same LXCPU,
and `.rl` is analogous to a release barrier.

Scope: unless a platform profile states otherwise, `.aq/.rl` apply to **Normal**
memory ordering; ordering with respect to **Device/MMIO** requires selecting
`O` in `FENCE.D` (or using platform-defined stronger rules).

[[memory-fences]]
=== Fences

The v0.3 staged catalog defines two fence instructions:

* `FENCE.D pred_imm, succ_imm`: orders memory operations.
* `FENCE.I`: synchronizes instruction fetch with prior data stores (self-modifying code / I-cache coherence boundary).

[[memory-fence-d]]
==== `FENCE.D pred_imm, succ_imm`

`pred_imm` and `succ_imm` are **bitmaps** selecting classes of operations.

Bring-up definition (v0.3 staged):

* Bit 0 (`R`): normal memory reads
* Bit 1 (`W`): normal memory writes
* Bit 2 (`O`): device/MMIO accesses
* Bit 3 (`I`): instruction-fetch visibility (see `FENCE.I`)

Semantics (v0.3 staged):

* `FENCE.D` orders all prior operations selected by `pred_imm` before all later
  operations selected by `succ_imm` in program order on the same LXCPU.
* If `O` is selected, `FENCE.D` orders device/MMIO accesses with respect to
  normal memory operations.
* If `I` is selected, `FENCE.D` additionally provides the instruction-fetch
  visibility ordering of `FENCE.I`.

Bring-up rule (cumulative visibility):

* `FENCE.D` is **cumulative**: if an LXCPU observes a write from another agent
  before the fence, then after the fence it MUST not observe later operations
  (selected by `succ_imm`) as if they occurred before that write. This matches
  the intent of an Arm-style `DMB` barrier for the selected operation classes.

Recommended usage patterns:

* Release: use either `.rl` on the publishing atomic OR `FENCE.D W,RW` (where
  `RW` means `R|W`) before publishing a flag.
* Acquire: use either `.aq` on the consuming atomic OR `FENCE.D RW,R` after
  observing the flag.
* MMIO ordering: include `O` in `pred_imm` and/or `succ_imm` when ordering
  device accesses against Normal memory (e.g. `FENCE.D O,RW`).

[[memory-fence-i]]
==== `FENCE.I`

`FENCE.I` is the architectural boundary for self-modifying code and runtime code
generation. After `FENCE.I`, instruction fetch MUST observe any prior committed
stores from the same LXCPU to memory that may be executed as instructions.

In Arm-like terms, `FENCE.I` is analogous to an instruction-synchronization
barrier for the issuing LXCPU (comparable to `ISB`) with the additional
requirement that prior stores that modify executable code become visible to
instruction fetch after the fence.
